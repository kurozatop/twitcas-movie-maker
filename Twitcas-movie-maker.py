import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import cv2
import numpy as np
import threading
import time
import os
from datetime import timedelta
import subprocess
from PIL import Image, ImageTk
import json

class ThumbnailEditor:
    def __init__(self, parent, video_path, video_info, point_entries):
        self.parent = parent
        self.video_path = video_path
        self.video_info = video_info
        self.point_entries = point_entries
        
        self.window = tk.Toplevel(parent)
        self.window.title("å°å½¢è£œæ­£è¨­å®š")
        self.window.geometry("800x700")
        self.window.grab_set()  # ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ã™ã‚‹
        
        self.current_frame = None
        self.display_frame = None
        self.canvas = None
        self.points = []
        self.dragging_point = None
        self.scale_factor = 1.0
        
        self.setup_ui()
        self.load_initial_frame()
    
    def setup_ui(self):
        # æ™‚é–“é¸æŠãƒ•ãƒ¬ãƒ¼ãƒ 
        time_frame = tk.Frame(self.window)
        time_frame.pack(pady=10, fill='x', padx=10)
        
        tk.Label(time_frame, text="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹æ™‚é–“:").pack(side='left')
        self.preview_h = tk.Spinbox(time_frame, from_=0, to=23, width=3, format="%02.0f")
        self.preview_h.pack(side='left', padx=2)
        tk.Label(time_frame, text="æ™‚é–“").pack(side='left')
        self.preview_m = tk.Spinbox(time_frame, from_=0, to=59, width=3, format="%02.0f")
        self.preview_m.pack(side='left', padx=2)
        tk.Label(time_frame, text="åˆ†").pack(side='left')
        self.preview_s = tk.Spinbox(time_frame, from_=0, to=59, width=3, format="%02.0f")
        self.preview_s.pack(side='left', padx=2)
        tk.Label(time_frame, text="ç§’").pack(side='left')
        self.preview_ms = tk.Spinbox(time_frame, from_=0, to=999, width=4, format="%03.0f")
        self.preview_ms.pack(side='left', padx=2)
        tk.Label(time_frame, text="ãƒŸãƒªç§’").pack(side='left')
        
        tk.Button(time_frame, text="ãƒ•ãƒ¬ãƒ¼ãƒ æ›´æ–°", command=self.update_frame).pack(side='left', padx=10)
        
        # èª¬æ˜æ–‡
        info_label = tk.Label(self.window, text="â€» é’ã„ç‚¹ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ã¦å°å½¢ã®4ã¤ã®è§’ã‚’èª¿æ•´ã—ã¦ãã ã•ã„", fg='blue')
        info_label.pack(pady=5)
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ 
        canvas_frame = tk.Frame(self.window)
        canvas_frame.pack(pady=10, fill='both', expand=True, padx=10)
        
        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ä»˜ãã‚­ãƒ£ãƒ³ãƒã‚¹
        self.canvas = tk.Canvas(canvas_frame, bg='black')
        v_scrollbar = ttk.Scrollbar(canvas_frame, orient="vertical", command=self.canvas.yview)
        h_scrollbar = ttk.Scrollbar(canvas_frame, orient="horizontal", command=self.canvas.xview)
        self.canvas.configure(yscrollcommand=v_scrollbar.set, xscrollcommand=h_scrollbar.set)
        
        v_scrollbar.pack(side="right", fill="y")
        h_scrollbar.pack(side="bottom", fill="x")
        self.canvas.pack(side="left", fill="both", expand=True)
        
        # ãƒã‚¦ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒã‚¤ãƒ³ãƒ‰
        self.canvas.bind("<Button-1>", self.on_canvas_click)
        self.canvas.bind("<B1-Motion>", self.on_canvas_drag)
        self.canvas.bind("<ButtonRelease-1>", self.on_canvas_release)
        
        # ãƒœã‚¿ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ 
        button_frame = tk.Frame(self.window)
        button_frame.pack(pady=10)
        
        tk.Button(button_frame, text="ãƒªã‚»ãƒƒãƒˆ", command=self.reset_points).pack(side='left', padx=5)
        tk.Button(button_frame, text="ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", command=self.preview_correction).pack(side='left', padx=5)
        tk.Button(button_frame, text="é©ç”¨", command=self.apply_points, bg='lightgreen').pack(side='left', padx=5)
        tk.Button(button_frame, text="ã‚­ãƒ£ãƒ³ã‚»ãƒ«", command=self.cancel).pack(side='left', padx=5)
    
    def load_initial_frame(self):
        """åˆæœŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿è¾¼ã¿"""
        self.update_frame()
    
    def get_preview_time(self):
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚é–“ã‚’ç§’ã§å–å¾—ï¼ˆãƒŸãƒªç§’å¯¾å¿œï¼‰"""
        return (int(self.preview_h.get()) * 3600 + 
                int(self.preview_m.get()) * 60 + 
                int(self.preview_s.get()) + 
                int(self.preview_ms.get()) / 1000.0)
    
    def update_frame(self):
        """æŒ‡å®šæ™‚é–“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ›´æ–°"""
        try:
            preview_time = self.get_preview_time()
            if preview_time > self.video_info['duration']:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "æŒ‡å®šæ™‚é–“ãŒå‹•ç”»ã®é•·ã•ã‚’è¶…ãˆã¦ã„ã¾ã™")
                return
            
            cap = cv2.VideoCapture(self.video_path)
            
            # æŒ‡å®šæ™‚é–“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã«ç§»å‹•
            frame_number = int(preview_time * self.video_info['fps'])
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            
            ret, frame = cap.read()
            cap.release()
            
            if ret:
                self.current_frame = frame
                self.display_frame_on_canvas()
                self.initialize_points()
            else:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "ãƒ•ãƒ¬ãƒ¼ãƒ ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ãƒ¬ãƒ¼ãƒ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def display_frame_on_canvas(self):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ³ãƒã‚¹ã«è¡¨ç¤º"""
        if self.current_frame is None:
            return
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’RGBã«å¤‰æ›
        frame_rgb = cv2.cvtColor(self.current_frame, cv2.COLOR_BGR2RGB)
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—
        canvas_width = 700
        canvas_height = 500
        
        frame_height, frame_width = frame_rgb.shape[:2]
        
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—
        scale_w = canvas_width / frame_width
        scale_h = canvas_height / frame_height
        self.scale_factor = min(scale_w, scale_h)
        
        new_width = int(frame_width * self.scale_factor)
        new_height = int(frame_height * self.scale_factor)
        
        # ãƒªã‚µã‚¤ã‚º
        frame_resized = cv2.resize(frame_rgb, (new_width, new_height))
        
        # PIL Imageã«å¤‰æ›
        pil_image = Image.fromarray(frame_resized)
        self.display_frame = ImageTk.PhotoImage(pil_image)
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚¯ãƒªã‚¢ã—ã¦ç”»åƒè¡¨ç¤º
        self.canvas.delete("all")
        self.canvas.create_image(0, 0, anchor="nw", image=self.display_frame)
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é ˜åŸŸã‚’è¨­å®š
        self.canvas.configure(scrollregion=self.canvas.bbox("all"))
    
    def initialize_points(self):
        """åˆæœŸã®4ç‚¹ã‚’è¨­å®š"""
        if self.current_frame is None:
            return
        
        frame_height, frame_width = self.current_frame.shape[:2]
        
        # ç¾åœ¨ã®åº§æ¨™ã‚’å–å¾—ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‹ã‚‰ï¼‰
        try:
            current_points = [
                (float(self.point_entries[0][0].get()), float(self.point_entries[0][1].get())),  # å·¦ä¸Š
                (float(self.point_entries[1][0].get()), float(self.point_entries[1][1].get())),  # å³ä¸Š
                (float(self.point_entries[2][0].get()), float(self.point_entries[2][1].get())),  # å·¦ä¸‹
                (float(self.point_entries[3][0].get()), float(self.point_entries[3][1].get()))   # å³ä¸‹
            ]
        except:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
            current_points = [
                (0, 0),
                (frame_width, 0),
                (0, frame_height),
                (frame_width, frame_height)
            ]
        
        # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã—ãŸåº§æ¨™ã«å¤‰æ›
        self.points = []
        for x, y in current_points:
            scaled_x = x * self.scale_factor
            scaled_y = y * self.scale_factor
            self.points.append([scaled_x, scaled_y])
        
        self.draw_points()
    
    def draw_points(self):
        """4ç‚¹ã¨å°å½¢ã‚’æç”»"""
        # æ—¢å­˜ã®ç‚¹ã¨ç·šã‚’å‰Šé™¤
        self.canvas.delete("point")
        self.canvas.delete("line")
        
        if len(self.points) != 4:
            return
        
        # å°å½¢ã®ç·šã‚’æç”»
        for i in range(4):
            start = self.points[i]
            end = self.points[(i + 1) % 4]
            self.canvas.create_line(start[0], start[1], end[0], end[1], 
                                  fill="red", width=2, tags="line")
        
        # ç‚¹ã‚’æç”»
        point_labels = ["å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"]
        for i, (x, y) in enumerate(self.points):
            # ç‚¹ã®å††
            self.canvas.create_oval(x-8, y-8, x+8, y+8, 
                                  fill="blue", outline="white", width=2, tags="point")
            # ãƒ©ãƒ™ãƒ«
            self.canvas.create_text(x, y-15, text=point_labels[i], 
                                  fill="yellow", font=("Arial", 10, "bold"), tags="point")
    
    def on_canvas_click(self, event):
        """ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚¯ãƒªãƒƒã‚¯æ™‚ã®å‡¦ç†"""
        # æœ€ã‚‚è¿‘ã„ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹
        click_x = self.canvas.canvasx(event.x)
        click_y = self.canvas.canvasy(event.y)
        
        min_distance = float('inf')
        closest_point = None
        
        for i, (x, y) in enumerate(self.points):
            distance = ((click_x - x) ** 2 + (click_y - y) ** 2) ** 0.5
            if distance < 20 and distance < min_distance:  # 20ãƒ”ã‚¯ã‚»ãƒ«ä»¥å†…
                min_distance = distance
                closest_point = i
        
        self.dragging_point = closest_point
    
    def on_canvas_drag(self, event):
        """ãƒ‰ãƒ©ãƒƒã‚°æ™‚ã®å‡¦ç†"""
        if self.dragging_point is not None:
            # æ–°ã—ã„åº§æ¨™ã‚’å–å¾—
            new_x = self.canvas.canvasx(event.x)
            new_y = self.canvas.canvasy(event.y)
            
            # ç”»åƒã®ç¯„å›²å†…ã«åˆ¶é™
            if self.display_frame:
                new_x = max(0, min(new_x, self.display_frame.width()))
                new_y = max(0, min(new_y, self.display_frame.height()))
            
            # ç‚¹ã®åº§æ¨™ã‚’æ›´æ–°
            self.points[self.dragging_point] = [new_x, new_y]
            self.draw_points()
    
    def on_canvas_release(self, event):
        """ãƒ‰ãƒ©ãƒƒã‚°çµ‚äº†æ™‚ã®å‡¦ç†"""
        self.dragging_point = None
    
    def reset_points(self):
        """ç‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        if self.current_frame is None:
            return
        
        frame_height, frame_width = self.current_frame.shape[:2]
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåº§æ¨™ã‚’ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨ã—ã¦è¨­å®š
        default_points = [
            (0, 0),
            (frame_width, 0),
            (0, frame_height),
            (frame_width, frame_height)
        ]
        
        self.points = []
        for x, y in default_points:
            scaled_x = x * self.scale_factor
            scaled_y = y * self.scale_factor
            self.points.append([scaled_x, scaled_y])
        
        self.draw_points()
    
    def preview_correction(self):
        """å°å½¢è£œæ­£ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º"""
        if self.current_frame is None or len(self.points) != 4:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "ãƒ•ãƒ¬ãƒ¼ãƒ ã¾ãŸã¯ç‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        try:
            # å…ƒã®åº§æ¨™ã«å¤‰æ›
            src_points = np.float32([
                [p[0] / self.scale_factor, p[1] / self.scale_factor] for p in self.points
            ])
            
            frame_height, frame_width = self.current_frame.shape[:2]
            dst_points = np.float32([
                [0, 0],
                [frame_width, 0],
                [0, frame_height],
                [frame_width, frame_height]
            ])
            
            # é€è¦–å¤‰æ›ã‚’é©ç”¨
            matrix = cv2.getPerspectiveTransform(src_points, dst_points)
            corrected = cv2.warpPerspective(self.current_frame, matrix, (frame_width, frame_height))
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½œæˆ
            preview_window = tk.Toplevel(self.window)
            preview_window.title("å°å½¢è£œæ­£ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            preview_window.geometry("800x600")
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã‚­ãƒ£ãƒ³ãƒã‚¹
            preview_canvas = tk.Canvas(preview_window, bg='black')
            preview_canvas.pack(fill='both', expand=True)
            
            # è£œæ­£å¾Œç”»åƒã‚’è¡¨ç¤º
            corrected_rgb = cv2.cvtColor(corrected, cv2.COLOR_BGR2RGB)
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ãƒªã‚µã‚¤ã‚º
            preview_scale = min(750 / frame_width, 550 / frame_height)
            preview_width = int(frame_width * preview_scale)
            preview_height = int(frame_height * preview_scale)
            
            corrected_resized = cv2.resize(corrected_rgb, (preview_width, preview_height))
            pil_preview = Image.fromarray(corrected_resized)
            preview_image = ImageTk.PhotoImage(pil_preview)
            
            preview_canvas.create_image(10, 10, anchor="nw", image=preview_image)
            
            # ç”»åƒã®å‚ç…§ã‚’ä¿æŒ
            preview_canvas.image = preview_image
            
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def apply_points(self):
        """è¨­å®šã—ãŸç‚¹ã‚’ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«é©ç”¨"""
        if len(self.points) != 4:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "4ã¤ã®ç‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        # å…ƒã®åº§æ¨™ã«å¤‰æ›ã—ã¦ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«è¨­å®š
        for i, (x, y) in enumerate(self.points):
            original_x = x / self.scale_factor
            original_y = y / self.scale_factor
            
            self.point_entries[i][0].delete(0, tk.END)
            self.point_entries[i][0].insert(0, str(int(original_x)))
            self.point_entries[i][1].delete(0, tk.END)
            self.point_entries[i][1].insert(0, str(int(original_y)))
        
        messagebox.showinfo("å®Œäº†", "å°å½¢è£œæ­£ã®åº§æ¨™ãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ")
        self.window.destroy()
    
    def cancel(self):
        """ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        self.window.destroy()


class VideoEditor:
    def __init__(self, root):
        self.root = root
        self.root.title("è¶…ç°¡å˜å‹•ç”»ç·¨é›†ã‚¢ãƒ—ãƒª (GPUå¯¾å¿œ)")
        self.root.geometry("650x850")  # ç¸¦ã‚’100ãƒ”ã‚¯ã‚»ãƒ«æ‹¡å¤§
        
        self.video_path = None
        self.video_info = None
        self.perspective_points = []
        self.available_gpus = self.detect_gpu_support()
        
        self.setup_ui()
    
    def detect_gpu_support(self):
        """åˆ©ç”¨å¯èƒ½ãªGPUã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ¤œå‡º"""
        gpu_options = []
        
        try:
            # ffmpegã®å­˜åœ¨ç¢ºèª
            result = subprocess.run(['ffmpeg', '-version'], 
                                  capture_output=True, text=True, 
                                  creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                  timeout=10)
            
            if result.returncode != 0:
                print("ffmpegãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return [('CPU (OpenCV)', 'opencv')]
            
            print("ffmpegæ¤œå‡ºæˆåŠŸ")
            
            # NVIDIA GPUæƒ…å ±ã‚’è©³ã—ãç¢ºèª
            try:
                nvidia_smi = subprocess.run(['nvidia-smi'], capture_output=True, text=True,
                                          creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                          timeout=10)
                if nvidia_smi.returncode == 0:
                    print("NVIDIA GPUæ¤œå‡º:", nvidia_smi.stdout.split('\n')[8:12])  # GPUæƒ…å ±è¡Œ
                else:
                    print("nvidia-smiå®Ÿè¡Œå¤±æ•—ã¾ãŸã¯NVIDIA GPUãªã—")
            except:
                print("nvidia-smi not found - NVIDIA GPUãŒç„¡ã„ã‹ã€ãƒ‰ãƒ©ã‚¤ãƒãƒ¼æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¸€è¦§ã‚’å–å¾—
            result = subprocess.run(['ffmpeg', '-encoders'], 
                                  capture_output=True, text=True, 
                                  creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                  timeout=10)
            encoders = result.stdout
            print("åˆ©ç”¨å¯èƒ½ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ç¢ºèªä¸­...")
            
            # NVENCè©³ç´°ãƒ†ã‚¹ãƒˆ
            if 'h264_nvenc' in encoders:
                print("h264_nvenc ãŒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒªã‚¹ãƒˆã«å­˜åœ¨")
                
                # ã‚ˆã‚Šè©³ç´°ãªNVENCãƒ†ã‚¹ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼æƒ…å ±ä»˜ãï¼‰
                test_cmd = ['ffmpeg', '-f', 'lavfi', '-i', 'testsrc=duration=0.1:size=320x240:rate=1', 
                           '-c:v', 'h264_nvenc', '-preset', 'fast', '-f', 'null', '-', '-v', 'error']
                try:
                    test_result = subprocess.run(test_cmd, capture_output=True, text=True, 
                                               creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                               timeout=15)
                    
                    if test_result.returncode == 0:
                        gpu_options.append(('NVIDIA GPU (H.264)', 'h264_nvenc'))
                        print("âœ… NVENC H.264 ä½¿ç”¨å¯èƒ½")
                    else:
                        print(f"âŒ NVENC H.264 ãƒ†ã‚¹ãƒˆå¤±æ•—:")
                        print(f"stderr: {test_result.stderr}")
                        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼åˆ†æ
                        if "Driver does not support the required nvenc API version" in test_result.stderr:
                            print("â†’ NVIDIAãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒå¤ã™ãã¾ã™")
                        elif "Cannot load nvcuda.dll" in test_result.stderr:
                            print("â†’ CUDA ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®å•é¡Œ")
                        elif "No NVENC capable devices found" in test_result.stderr:
                            print("â†’ NVENCå¯¾å¿œGPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        else:
                            print("â†’ ãã®ä»–ã®NVENCå•é¡Œ")
                            
                except subprocess.TimeoutExpired:
                    print("âŒ NVENC ãƒ†ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                except Exception as e:
                    print(f"âŒ NVENC ãƒ†ã‚¹ãƒˆä¾‹å¤–: {e}")
            else:
                print("h264_nvenc ãŒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒªã‚¹ãƒˆã«å­˜åœ¨ã—ã¾ã›ã‚“")
            
            # H.265 NVENC ãƒ†ã‚¹ãƒˆ
            if 'hevc_nvenc' in encoders:
                test_cmd = ['ffmpeg', '-f', 'lavfi', '-i', 'testsrc=duration=0.1:size=320x240:rate=1', 
                           '-c:v', 'hevc_nvenc', '-preset', 'fast', '-f', 'null', '-', '-v', 'error']
                try:
                    test_result = subprocess.run(test_cmd, capture_output=True, text=True, 
                                               creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                               timeout=15)
                    if test_result.returncode == 0:
                        gpu_options.append(('NVIDIA GPU (H.265)', 'hevc_nvenc'))
                        print("âœ… NVENC H.265 ä½¿ç”¨å¯èƒ½")
                    else:
                        print(f"âŒ NVENC H.265 ãƒ†ã‚¹ãƒˆå¤±æ•—: {test_result.stderr}")
                except:
                    print("âŒ NVENC H.265 ãƒ†ã‚¹ãƒˆä¾‹å¤–")
            
            # Intel QuickSync ãƒ†ã‚¹ãƒˆ
            if 'h264_qsv' in encoders:
                test_cmd = ['ffmpeg', '-f', 'lavfi', '-i', 'testsrc=duration=0.1:size=320x240:rate=1', 
                           '-c:v', 'h264_qsv', '-f', 'null', '-', '-v', 'error']
                try:
                    test_result = subprocess.run(test_cmd, capture_output=True, text=True, 
                                               creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                               timeout=15)
                    if test_result.returncode == 0:
                        gpu_options.append(('Intel QuickSync (H.264)', 'h264_qsv'))
                        print("âœ… Intel QSV H.264 ä½¿ç”¨å¯èƒ½")
                    else:
                        print(f"âŒ Intel QSV H.264 ä½¿ç”¨ä¸å¯: {test_result.stderr}")
                except:
                    print("âŒ Intel QSV ãƒ†ã‚¹ãƒˆä¾‹å¤–")
                    
            if 'hevc_qsv' in encoders:
                gpu_options.append(('Intel QuickSync (H.265)', 'hevc_qsv'))
            
            # CPU ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
            if 'libx264' in encoders:
                gpu_options.append(('CPU (H.264)', 'libx264'))
                print("âœ… CPU H.264 ä½¿ç”¨å¯èƒ½")
                
        except subprocess.TimeoutExpired:
            print("ffmpegã‚³ãƒãƒ³ãƒ‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        except FileNotFoundError:
            print("ffmpegãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"GPUæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        # OpenCV ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæœ€ã‚‚å®‰å…¨ï¼‰
        gpu_options.append(('CPU (OpenCV)', 'opencv'))
        print("âœ… CPU OpenCV ä½¿ç”¨å¯èƒ½")
        
        return gpu_options if gpu_options else [('CPU (OpenCV)', 'opencv')]
    
    def setup_ui(self):
        # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        file_frame = tk.Frame(self.root)
        file_frame.pack(pady=10, fill='x', padx=10)
        
        tk.Button(file_frame, text="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", command=self.select_video).pack(side='left')
        self.file_label = tk.Label(file_frame, text="ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“", fg='gray')
        self.file_label.pack(side='left', padx=(10, 0))
        
        # GPUè¨­å®šã¨ãƒ‡ã‚£agnostic
        gpu_frame = tk.LabelFrame(self.root, text="ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¨­å®š")
        gpu_frame.pack(pady=10, fill='x', padx=10)
        
        tk.Label(gpu_frame, text="ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼:").pack(anchor='w', padx=10, pady=2)
        self.encoder_var = tk.StringVar()
        self.encoder_combo = ttk.Combobox(gpu_frame, textvariable=self.encoder_var, 
                                         values=[option[0] for option in self.available_gpus],
                                         state='readonly')
        self.encoder_combo.pack(fill='x', padx=10, pady=2)
        if self.available_gpus:
            self.encoder_combo.current(0)  # æœ€åˆã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
        
        # è¨ºæ–­ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
        diag_frame = tk.Frame(gpu_frame)
        diag_frame.pack(fill='x', padx=10, pady=2)
        tk.Button(diag_frame, text="ğŸ” GPUè¨ºæ–­", command=self.show_gpu_diagnostics, 
                 bg='lightcyan').pack(side='left')
        self.diag_label = tk.Label(diag_frame, text="", fg='blue', font=('Arial', 8))
        self.diag_label.pack(side='left', padx=10)
        
        # å“è³ªè¨­å®š
        quality_frame = tk.Frame(gpu_frame)
        quality_frame.pack(fill='x', padx=10, pady=5)
        tk.Label(quality_frame, text="å“è³ª:").pack(side='left')
        self.quality_var = tk.StringVar(value="é«˜å“è³ª")
        quality_combo = ttk.Combobox(quality_frame, textvariable=self.quality_var,
                                   values=["æœ€é«˜å“è³ª", "é«˜å“è³ª", "æ¨™æº–å“è³ª", "é«˜é€Ÿ"],
                                   state='readonly', width=15)
        quality_combo.pack(side='left', padx=5)
        quality_combo.current(1)
        
        # å‹•ç”»æƒ…å ±è¡¨ç¤º
        info_frame = tk.LabelFrame(self.root, text="å‹•ç”»æƒ…å ±")
        info_frame.pack(pady=10, fill='x', padx=10)
        
        self.duration_label = tk.Label(info_frame, text="åˆè¨ˆæ™‚é–“: --")
        self.duration_label.pack(anchor='w', padx=10, pady=5)
        
        self.resolution_label = tk.Label(info_frame, text="ç”»è³ª: --")
        self.resolution_label.pack(anchor='w', padx=10, pady=5)
        
        # åˆ‡ã‚ŠæŠœãè¨­å®š
        trim_frame = tk.LabelFrame(self.root, text="åˆ‡ã‚ŠæŠœãè¨­å®š")
        trim_frame.pack(pady=10, fill='x', padx=10)
        
        start_frame = tk.Frame(trim_frame)
        start_frame.pack(fill='x', padx=10, pady=5)
        tk.Label(start_frame, text="é–‹å§‹æ™‚é–“:").pack(side='left')
        self.start_h = tk.Spinbox(start_frame, from_=0, to=23, width=3, format="%02.0f")
        self.start_h.pack(side='left', padx=2)
        tk.Label(start_frame, text="æ™‚é–“").pack(side='left')
        self.start_m = tk.Spinbox(start_frame, from_=0, to=59, width=3, format="%02.0f")
        self.start_m.pack(side='left', padx=2)
        tk.Label(start_frame, text="åˆ†").pack(side='left')
        self.start_s = tk.Spinbox(start_frame, from_=0, to=59, width=3, format="%02.0f")
        self.start_s.pack(side='left', padx=2)
        tk.Label(start_frame, text="ç§’").pack(side='left')
        self.start_ms = tk.Spinbox(start_frame, from_=0, to=999, width=4, format="%03.0f")
        self.start_ms.pack(side='left', padx=2)
        tk.Label(start_frame, text="ãƒŸãƒªç§’").pack(side='left')
        
        end_frame = tk.Frame(trim_frame)
        end_frame.pack(fill='x', padx=10, pady=5)
        tk.Label(end_frame, text="çµ‚äº†æ™‚é–“:").pack(side='left')
        self.end_h = tk.Spinbox(end_frame, from_=0, to=23, width=3, format="%02.0f")
        self.end_h.pack(side='left', padx=2)
        tk.Label(end_frame, text="æ™‚é–“").pack(side='left')
        self.end_m = tk.Spinbox(end_frame, from_=0, to=59, width=3, format="%02.0f")
        self.end_m.pack(side='left', padx=2)
        tk.Label(end_frame, text="åˆ†").pack(side='left')
        self.end_s = tk.Spinbox(end_frame, from_=0, to=59, width=3, format="%02.0f")
        self.end_s.pack(side='left', padx=2)
        tk.Label(end_frame, text="ç§’").pack(side='left')
        self.end_ms = tk.Spinbox(end_frame, from_=0, to=999, width=4, format="%03.0f")
        self.end_ms.pack(side='left', padx=2)
        tk.Label(end_frame, text="ãƒŸãƒªç§’").pack(side='left')
        
        # å°å½¢è£œæ­£è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        perspective_frame = tk.LabelFrame(self.root, text="å°å½¢è£œæ­£è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        perspective_frame.pack(pady=10, fill='x', padx=10)
        
        self.use_perspective = tk.BooleanVar()
        tk.Checkbutton(perspective_frame, text="å°å½¢è£œæ­£ã‚’ä½¿ç”¨", variable=self.use_perspective).pack(anchor='w', padx=10, pady=5)
        
        # è¦–è¦šçš„è¨­å®šãƒœã‚¿ãƒ³
        visual_button_frame = tk.Frame(perspective_frame)
        visual_button_frame.pack(fill='x', padx=10, pady=5)
        
        self.visual_button = tk.Button(visual_button_frame, text="ğŸ“· è¦–è¦šçš„ã«è¨­å®š", 
                                     command=self.open_thumbnail_editor, bg='lightblue',
                                     state='disabled')
        self.visual_button.pack(side='left')
        
        tk.Label(visual_button_frame, text="â† å‹•ç”»ã‚’é¸æŠå¾Œã«ä½¿ç”¨å¯èƒ½", fg='gray').pack(side='left', padx=10)
        
        points_frame = tk.Frame(perspective_frame)
        points_frame.pack(fill='x', padx=10, pady=5)
        
        # 4ç‚¹ã®åº§æ¨™å…¥åŠ›
        self.point_entries = []
        point_labels = ["å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"]
        for i, label in enumerate(point_labels):
            point_frame = tk.Frame(points_frame)
            point_frame.pack(fill='x', pady=2)
            tk.Label(point_frame, text=f"{label}:", width=6).pack(side='left')
            tk.Label(point_frame, text="X:").pack(side='left')
            x_entry = tk.Entry(point_frame, width=6)
            x_entry.pack(side='left', padx=2)
            tk.Label(point_frame, text="Y:").pack(side='left')
            y_entry = tk.Entry(point_frame, width=6)
            y_entry.pack(side='left', padx=2)
            self.point_entries.append((x_entry, y_entry))
        
        # é€²æ—è¡¨ç¤º
        progress_frame = tk.LabelFrame(self.root, text="é€²æ—")
        progress_frame.pack(pady=10, fill='x', padx=10)
        
        self.progress_bar = ttk.Progressbar(progress_frame, mode='determinate')
        self.progress_bar.pack(fill='x', padx=10, pady=5)
        
        self.progress_label = tk.Label(progress_frame, text="å¾…æ©Ÿä¸­...")
        self.progress_label.pack(padx=10, pady=5)
        
        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        tk.Button(self.root, text="å‹•ç”»ã‚’å‡¦ç†", command=self.process_video, bg='lightgreen').pack(pady=20)
    
    def select_video(self):
        file_path = filedialog.askopenfilename(
            title="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            filetypes=[("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«", "*.mp4 *.avi *.mov *.mkv *.wmv")]
        )
        if file_path:
            self.video_path = file_path
            self.file_label.config(text=os.path.basename(file_path), fg='black')
            self.get_video_info()
            # è¦–è¦šçš„è¨­å®šãƒœã‚¿ãƒ³ã‚’æœ‰åŠ¹åŒ–
            self.visual_button.config(state='normal')
    
    def get_video_info(self):
        try:
            # OpenCVã‚’ä½¿ç”¨ã—ã¦å‹•ç”»æƒ…å ±ã‚’å–å¾—
            cap = cv2.VideoCapture(self.video_path)
            
            if not cap.isOpened():
                raise Exception("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“")
            
            # å‹•ç”»æƒ…å ±ã‚’å–å¾—
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # ç·æ™‚é–“ã‚’è¨ˆç®—
            duration = frame_count / fps if fps > 0 else 0
            duration_str = str(timedelta(seconds=int(duration)))
            
            cap.release()
            
            # æƒ…å ±ã‚’è¡¨ç¤º
            self.duration_label.config(text=f"åˆè¨ˆæ™‚é–“: {duration_str}")
            self.resolution_label.config(text=f"ç”»è³ª: {width}x{height}")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å°å½¢è£œæ­£åº§æ¨™ã‚’è¨­å®š
            default_coords = [
                ("0", "0"),           # å·¦ä¸Š
                (str(width), "0"),    # å³ä¸Š
                ("0", str(height)),   # å·¦ä¸‹
                (str(width), str(height))  # å³ä¸‹
            ]
            
            for i, (x, y) in enumerate(default_coords):
                self.point_entries[i][0].delete(0, tk.END)
                self.point_entries[i][0].insert(0, x)
                self.point_entries[i][1].delete(0, tk.END)
                self.point_entries[i][1].insert(0, y)
            
            self.video_info = {
                'width': width,
                'height': height,
                'fps': fps,
                'duration': duration,
                'frame_count': frame_count
            }
            
        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            print(f"è©³ç´°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def show_gpu_diagnostics(self):
        """GPUè¨ºæ–­æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦"""
        diag_window = tk.Toplevel(self.root)
        diag_window.title("GPUè¨ºæ–­æƒ…å ±")
        diag_window.geometry("700x500")
        
        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
        text_frame = tk.Frame(diag_window)
        text_frame.pack(fill='both', expand=True, padx=10, pady=10)
        
        scrollbar = tk.Scrollbar(text_frame)
        scrollbar.pack(side='right', fill='y')
        
        text_area = tk.Text(text_frame, yscrollcommand=scrollbar.set, font=('Consolas', 9))
        text_area.pack(side='left', fill='both', expand=True)
        scrollbar.config(command=text_area.yview)
        
        # è¨ºæ–­æƒ…å ±ã‚’åé›†
        diag_info = "=== GPUè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ ===\n\n"
        
        try:
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
            diag_info += "1. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\n"
            diag_info += f"OS: {os.name}\n"
            
            # NVIDIA GPUç¢ºèª
            diag_info += "\n2. NVIDIA GPUç¢ºèª\n"
            try:
                nvidia_smi = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,cuda_version', '--format=csv'], 
                                          capture_output=True, text=True,
                                          creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                          timeout=10)
                if nvidia_smi.returncode == 0:
                    diag_info += nvidia_smi.stdout
                else:
                    diag_info += "nvidia-smiå®Ÿè¡Œå¤±æ•—\n"
            except FileNotFoundError:
                diag_info += "nvidia-smi not found (NVIDIAãƒ‰ãƒ©ã‚¤ãƒãƒ¼æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«?)\n"
            except Exception as e:
                diag_info += f"nvidia-smi ã‚¨ãƒ©ãƒ¼: {e}\n"
            
            # ffmpeg ãƒãƒ¼ã‚¸ãƒ§ãƒ³
            diag_info += "\n3. ffmpegæƒ…å ±\n"
            try:
                ffmpeg_ver = subprocess.run(['ffmpeg', '-version'], 
                                          capture_output=True, text=True,
                                          creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                          timeout=10)
                if ffmpeg_ver.returncode == 0:
                    # æœ€åˆã®æ•°è¡Œã®ã¿è¡¨ç¤º
                    lines = ffmpeg_ver.stdout.split('\n')[:5]
                    diag_info += '\n'.join(lines) + '\n'
                else:
                    diag_info += "ffmpeg ãƒãƒ¼ã‚¸ãƒ§ãƒ³å–å¾—å¤±æ•—\n"
            except Exception as e:
                diag_info += f"ffmpeg ã‚¨ãƒ©ãƒ¼: {e}\n"
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ç¢ºèª
            diag_info += "\n4. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ç¢ºèª\n"
            try:
                encoders = subprocess.run(['ffmpeg', '-encoders'], 
                                        capture_output=True, text=True,
                                        creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                        timeout=10)
                if encoders.returncode == 0:
                    # NVENCé–¢é€£ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ã¿æŠ½å‡º
                    for line in encoders.stdout.split('\n'):
                        if 'nvenc' in line.lower() or 'h264' in line.lower() or 'hevc' in line.lower():
                            diag_info += line + '\n'
                else:
                    diag_info += "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¸€è¦§å–å¾—å¤±æ•—\n"
            except Exception as e:
                diag_info += f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\n"
            
            # NVENCè©³ç´°ãƒ†ã‚¹ãƒˆ
            diag_info += "\n5. NVENCè©³ç´°ãƒ†ã‚¹ãƒˆ\n"
            test_cmd = ['ffmpeg', '-f', 'lavfi', '-i', 'testsrc=duration=0.1:size=320x240:rate=1', 
                       '-c:v', 'h264_nvenc', '-preset', 'fast', '-f', 'null', '-']
            try:
                test_result = subprocess.run(test_cmd, capture_output=True, text=True,
                                           creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
                                           timeout=15)
                diag_info += f"ãƒ†ã‚¹ãƒˆã‚³ãƒãƒ³ãƒ‰: {' '.join(test_cmd)}\n"
                diag_info += f"æˆ»ã‚Šå€¤: {test_result.returncode}\n"
                if test_result.stderr:
                    diag_info += f"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:\n{test_result.stderr}\n"
                if test_result.stdout:
                    diag_info += f"æ¨™æº–å‡ºåŠ›:\n{test_result.stdout}\n"
            except Exception as e:
                diag_info += f"NVENCãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}\n"
            
            # æ¨å¥¨è§£æ±ºç­–
            diag_info += "\n6. æ¨å¥¨è§£æ±ºç­–\n"
            diag_info += "A. NVIDIAãƒ‰ãƒ©ã‚¤ãƒãƒ¼æ›´æ–°:\n"
            diag_info += "   - https://www.nvidia.com/drivers/ ã‹ã‚‰æœ€æ–°ç‰ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
            diag_info += "   - GeForce ExperienceçµŒç”±ã§ã‚‚æ›´æ–°å¯èƒ½\n\n"
            diag_info += "B. CUDA Toolkitã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:\n"
            diag_info += "   - https://developer.nvidia.com/cuda-downloads\n\n"
            diag_info += "C. ä»£æ›¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½¿ç”¨:\n"
            diag_info += "   - 'CPU (H.264)' ã¾ãŸã¯ 'CPU (OpenCV)' ã‚’é¸æŠ\n"
            diag_info += "   - å“è³ªã¯åŒç­‰ã€é€Ÿåº¦ã¯è‹¥å¹²é…ããªã‚Šã¾ã™\n\n"
            
        except Exception as e:
            diag_info += f"\nè¨ºæ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}\n"
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«è¡¨ç¤º
        text_area.insert('1.0', diag_info)
        text_area.config(state='disabled')  # èª­ã¿å–ã‚Šå°‚ç”¨
        
        # ç°¡æ˜“è¨ºæ–­çµæœã‚’ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ã‚‚è¡¨ç¤º
        if "nvidia-smi not found" in diag_info:
            self.diag_label.config(text="NVIDIAãƒ‰ãƒ©ã‚¤ãƒãƒ¼æœªæ¤œå‡º")
        elif "æˆ»ã‚Šå€¤: 0" in diag_info:
            self.diag_label.config(text="NVENCåˆ©ç”¨å¯èƒ½", fg='green')
        else:
            self.diag_label.config(text="NVENCåˆ©ç”¨ä¸å¯ (è©³ç´°ã¯è¨ºæ–­ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‚ç…§)", fg='red')
    
    def open_thumbnail_editor(self):
        """ã‚µãƒ ãƒã‚¤ãƒ«ç·¨é›†ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‹ã"""
        if not self.video_path or not self.video_info:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            return
        
        ThumbnailEditor(self.root, self.video_path, self.video_info, self.point_entries)
    
    def get_time_in_seconds(self, h_spinbox, m_spinbox, s_spinbox, ms_spinbox=None):
        """æ™‚é–“ã‚’ç§’ã«å¤‰æ›ï¼ˆãƒŸãƒªç§’å¯¾å¿œï¼‰"""
        seconds = int(h_spinbox.get()) * 3600 + int(m_spinbox.get()) * 60 + int(s_spinbox.get())
        if ms_spinbox is not None:
            seconds += int(ms_spinbox.get()) / 1000.0
        return seconds
    
    def get_encoder_settings(self):
        """é¸æŠã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦è¨­å®šã‚’è¿”ã™"""
        encoder_name = self.encoder_var.get()
        encoder_code = None
        
        for name, code in self.available_gpus:
            if name == encoder_name:
                encoder_code = code
                break
        
        if not encoder_code:
            encoder_code = 'opencv'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        quality = self.quality_var.get()
        
        # OpenCVã®å ´åˆã¯å“è³ªè¨­å®šã‚’è¿”ã™
        if encoder_code == 'opencv':
            quality_map = {
                "æœ€é«˜å“è³ª": 95,
                "é«˜å“è³ª": 85,
                "æ¨™æº–å“è³ª": 75,
                "é«˜é€Ÿ": 65
            }
            return encoder_code, quality_map.get(quality, 85)
        
        # ffmpegç”¨ã®å“è³ªè¨­å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
        if 'nvenc' in encoder_code:
            # NVIDIA GPUè¨­å®š - ã‚ˆã‚Šå®‰å®šã—ãŸè¨­å®š
            quality_map = {
                "æœ€é«˜å“è³ª": ['-preset', 'slow', '-cq', '18'],
                "é«˜å“è³ª": ['-preset', 'medium', '-cq', '23'],
                "æ¨™æº–å“è³ª": ['-preset', 'fast', '-cq', '28'],
                "é«˜é€Ÿ": ['-preset', 'ultrafast', '-cq', '30']
            }
        elif 'qsv' in encoder_code:
            # Intel QuickSyncè¨­å®š
            quality_map = {
                "æœ€é«˜å“è³ª": ['-preset', 'veryslow', '-global_quality', '18'],
                "é«˜å“è³ª": ['-preset', 'medium', '-global_quality', '23'],
                "æ¨™æº–å“è³ª": ['-preset', 'fast', '-global_quality', '28'],
                "é«˜é€Ÿ": ['-preset', 'ultrafast', '-global_quality', '30']
            }
        else:
            # CPUè¨­å®š
            quality_map = {
                "æœ€é«˜å“è³ª": ['-preset', 'veryslow', '-crf', '18'],
                "é«˜å“è³ª": ['-preset', 'medium', '-crf', '23'],
                "æ¨™æº–å“è³ª": ['-preset', 'fast', '-crf', '28'],
                "é«˜é€Ÿ": ['-preset', 'ultrafast', '-crf', '30']
            }
        
        return encoder_code, quality_map.get(quality, quality_map["é«˜å“è³ª"])
    
    def process_video_opencv(self, output_path, start_time, end_time, quality):
        """OpenCVã‚’ä½¿ç”¨ã—ãŸå‹•ç”»å‡¦ç†"""
        try:
            cap = cv2.VideoCapture(self.video_path)
            
            if not cap.isOpened():
                raise Exception("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“")
            
            # å‹•ç”»æƒ…å ±
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # é–‹å§‹ãƒ»çµ‚äº†ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¨ˆç®—
            start_frame = int(start_time * fps)
            end_frame = int(end_time * fps)
            total_frames = end_frame - start_frame
            
            # é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ ã«ç§»å‹•
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # å‡ºåŠ›è¨­å®š
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            # å°å½¢è£œæ­£ã®æº–å‚™
            use_correction = self.use_perspective.get()
            if use_correction:
                try:
                    src_points = np.float32([
                        [float(self.point_entries[0][0].get()), float(self.point_entries[0][1].get())],  # å·¦ä¸Š
                        [float(self.point_entries[1][0].get()), float(self.point_entries[1][1].get())],  # å³ä¸Š
                        [float(self.point_entries[2][0].get()), float(self.point_entries[2][1].get())],  # å·¦ä¸‹
                        [float(self.point_entries[3][0].get()), float(self.point_entries[3][1].get())]   # å³ä¸‹
                    ])
                    dst_points = np.float32([
                        [0, 0],
                        [width, 0],
                        [0, height],
                        [width, height]
                    ])
                    perspective_matrix = cv2.getPerspectiveTransform(src_points, dst_points)
                except ValueError:
                    raise Exception("å°å½¢è£œæ­£ã®åº§æ¨™ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
            
            processed_frames = 0
            start_process_time = time.time()
            
            while processed_frames < total_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # å°å½¢è£œæ­£ã‚’é©ç”¨
                if use_correction:
                    frame = cv2.warpPerspective(frame, perspective_matrix, (width, height))
                
                out.write(frame)
                processed_frames += 1
                
                # é€²æ—ã‚’æ›´æ–°
                if processed_frames % 30 == 0:  # 30ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«æ›´æ–°
                    progress = (processed_frames / total_frames) * 100
                    elapsed_time = time.time() - start_process_time
                    if processed_frames > 0:
                        remaining_time = (elapsed_time / processed_frames) * (total_frames - processed_frames)
                        remaining_str = str(timedelta(seconds=int(remaining_time)))
                    else:
                        remaining_str = "è¨ˆç®—ä¸­..."
                    
                    self.root.after(0, lambda p=progress, r=remaining_str: 
                                  self.update_progress(p, f"å‡¦ç†ä¸­ - æ®‹ã‚Š: {r}"))
            
            cap.release()
            out.release()
            return True
            
        except Exception as e:
            print(f"OpenCVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def process_video_ffmpeg(self, output_path, start_time, end_time, encoder, quality_settings):
        """ffmpegã‚’ä½¿ç”¨ã—ãŸå‹•ç”»å‡¦ç†ï¼ˆéŸ³å£°å¯¾å¿œã€GPUæœ€é©åŒ–ï¼‰"""
        try:
            # å°å½¢è£œæ­£ãŒå¿…è¦ãªå ´åˆã¯ã€å…ˆã«ffmpegã§å°å½¢è£œæ­£ã‚’é©ç”¨
            if self.use_perspective.get():
                return self.process_video_ffmpeg_with_perspective(output_path, start_time, end_time, encoder, quality_settings)
            
            cmd = ['ffmpeg', '-y']  # -y ã§ä¸Šæ›¸ãç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—
            
            # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’æ§ãˆã‚ã«è¨­å®šï¼ˆäº’æ›æ€§é‡è¦–ï¼‰
            if 'nvenc' in encoder:
                # NVIDIAã®å ´åˆã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                pass  # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ä½¿ã‚ãªã„
            elif 'qsv' in encoder:
                # Intel QSVã‚‚åŒæ§˜
                pass
            
            # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨ç¯„å›²æŒ‡å®š
            cmd.extend(['-i', self.video_path, '-ss', str(start_time), '-t', str(end_time - start_time)])
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼è¨­å®š
            cmd.extend(['-c:v', encoder])
            
            # éŸ³å£°è¨­å®šï¼ˆå…ƒã®éŸ³å£°ã‚’ä¿æŒï¼‰
            cmd.extend(['-c:a', 'copy'])  # éŸ³å£°ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ï¼ˆé«˜é€Ÿï¼‹å“è³ªä¿æŒï¼‰
            
            # å“è³ªè¨­å®šã‚’è¿½åŠ 
            if quality_settings:
                cmd.extend(quality_settings)
            
            # ãƒ”ã‚¯ã‚»ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡å®šï¼ˆäº’æ›æ€§å‘ä¸Šï¼‰
            cmd.extend(['-pix_fmt', 'yuv420p'])
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
            cmd.append(output_path)
            
            print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            
            # ffmpegå®Ÿè¡Œ
            self.root.after(0, lambda: self.progress_label.config(text="ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­..."))
            
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
                                     creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
            
            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = process.communicate()
            
            if process.returncode != 0:
                error_msg = stderr.decode('utf-8', errors='ignore')
                print(f"ffmpegã‚¨ãƒ©ãƒ¼è©³ç´°: {error_msg}")
                raise Exception(f"ffmpegã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {error_msg}")
            
            print("ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except Exception as e:
            print(f"ffmpegå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def process_video_ffmpeg_with_perspective(self, output_path, start_time, end_time, encoder, quality_settings):
        """ffmpegã§å°å½¢è£œæ­£ã‚’å«ã‚€å‹•ç”»å‡¦ç†ï¼ˆGPUã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
        try:
            # å°å½¢è£œæ­£ã®åº§æ¨™ã‚’å–å¾—
            src_points = []
            try:
                for i in range(4):
                    x = float(self.point_entries[i][0].get())
                    y = float(self.point_entries[i][1].get())
                    src_points.append((x, y))
            except ValueError:
                raise Exception("å°å½¢è£œæ­£ã®åº§æ¨™ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
            
            # ã¾ãšã¯OpenCVã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’ç›´æ¥å®Ÿè¡Œ
            # ffmpegã®perspectiveãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¯è¤‡é›‘ã™ãã‚‹ãŸã‚
            print("å°å½¢è£œæ­£: OpenCVæ–¹å¼ã‚’ä½¿ç”¨ã—ã¾ã™")
            return self.process_video_opencv_fallback(output_path, start_time, end_time, encoder, quality_settings)
            
        except Exception as e:
            print(f"å°å½¢è£œæ­£å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚OpenCVã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print("OpenCVã§ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
            return self.process_video_opencv_fallback(output_path, start_time, end_time, encoder, quality_settings)
    
    def process_video_opencv_fallback(self, output_path, start_time, end_time, encoder, quality_settings):
        """OpenCVã§å°å½¢è£œæ­£ã‚’è¡Œã„ã€ãã®å¾Œffmpegã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            temp_video_path = output_path.replace('.mp4', '_corrected_temp.mp4')
            
            # OpenCVã§å°å½¢è£œæ­£ã‚’é©ç”¨
            self.root.after(0, lambda: self.progress_label.config(text="å°å½¢è£œæ­£ã‚’é©ç”¨ä¸­..."))
            
            src_points = np.float32([
                [float(self.point_entries[0][0].get()), float(self.point_entries[0][1].get())],  # å·¦ä¸Š
                [float(self.point_entries[1][0].get()), float(self.point_entries[1][1].get())],  # å³ä¸Š
                [float(self.point_entries[2][0].get()), float(self.point_entries[2][1].get())],  # å·¦ä¸‹
                [float(self.point_entries[3][0].get()), float(self.point_entries[3][1].get())]   # å³ä¸‹
            ])
            dst_points = np.float32([
                [0, 0],
                [self.video_info['width'], 0],
                [0, self.video_info['height']],
                [self.video_info['width'], self.video_info['height']]
            ])
            
            print(f"å°å½¢è£œæ­£åº§æ¨™:")
            print(f"  å…ƒåº§æ¨™: {src_points}")
            print(f"  å¤‰æ›å¾Œ: {dst_points}")
            
            cap = cv2.VideoCapture(self.video_path)
            
            # æŒ‡å®šç¯„å›²ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿å‡¦ç†
            start_frame = int(start_time * self.video_info['fps'])
            end_frame = int(end_time * self.video_info['fps'])
            total_frames = end_frame - start_frame
            
            print(f"å‡¦ç†ç¯„å›²: ãƒ•ãƒ¬ãƒ¼ãƒ  {start_frame} - {end_frame} (åˆè¨ˆ {total_frames} ãƒ•ãƒ¬ãƒ¼ãƒ )")
            
            # é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ ã«ç§»å‹•
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # ä¸€æ™‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆæ˜ åƒã®ã¿ã€é«˜å“è³ªï¼‰
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_video_path, fourcc, self.video_info['fps'],
                                (self.video_info['width'], self.video_info['height']))
            
            if not out.isOpened():
                raise Exception("ä¸€æ™‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            processed_frames = 0
            matrix = cv2.getPerspectiveTransform(src_points, dst_points)
            print(f"å¤‰æ›è¡Œåˆ—: \n{matrix}")
            
            while processed_frames < total_frames:
                ret, frame = cap.read()
                if not ret:
                    print(f"ãƒ•ãƒ¬ãƒ¼ãƒ  {processed_frames} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
                    break
                
                # å°å½¢è£œæ­£ã‚’é©ç”¨
                corrected = cv2.warpPerspective(frame, matrix, 
                                              (self.video_info['width'], self.video_info['height']))
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ãŒçœŸã£é»’ã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if processed_frames == 0:
                    mean_brightness = np.mean(corrected)
                    print(f"æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡è¼åº¦: {mean_brightness}")
                    if mean_brightness < 1:
                        print("è­¦å‘Š: è£œæ­£å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒçœŸã£é»’ã§ã™")
                
                out.write(corrected)
                processed_frames += 1
                
                if processed_frames % 30 == 0:
                    progress = (processed_frames / total_frames) * 50
                    self.root.after(0, lambda p=progress: self.update_progress(p, f"å°å½¢è£œæ­£ä¸­... {processed_frames}/{total_frames}"))
            
            cap.release()
            out.release()
            
            print(f"å°å½¢è£œæ­£å®Œäº†: {processed_frames} ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†")
            
            # ffmpegã§éŸ³å£°ã‚’çµåˆã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            self.root.after(0, lambda: self.progress_label.config(text="éŸ³å£°çµåˆ+ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ä¸­..."))
            
            cmd = ['ffmpeg', '-y',
                   '-i', temp_video_path,  # å°å½¢è£œæ­£æ¸ˆã¿æ˜ åƒ
                   '-ss', str(start_time), '-t', str(end_time - start_time),
                   '-i', self.video_path]  # å…ƒå‹•ç”»ï¼ˆéŸ³å£°ç”¨ï¼‰
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼è¨­å®š
            cmd.extend(['-c:v', encoder])
            
            # å“è³ªè¨­å®š
            if quality_settings:
                cmd.extend(quality_settings)
            
            # éŸ³å£°è¨­å®šã¨ãƒãƒƒãƒ”ãƒ³ã‚°
            cmd.extend([
                '-c:a', 'copy',
                '-map', '0:v:0',  # 1ç•ªç›®ã®å…¥åŠ›ã®æ˜ åƒï¼ˆå°å½¢è£œæ­£æ¸ˆã¿ï¼‰
                '-map', '1:a:0?',  # 2ç•ªç›®ã®å…¥åŠ›ã®éŸ³å£°ï¼ˆ?ã§éŸ³å£°ãŒãªã„å ´åˆã‚‚è¨±å¯ï¼‰
                '-pix_fmt', 'yuv420p',
                output_path
            ])
            
            print(f"éŸ³å£°çµåˆã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            
            process = subprocess.run(cmd, capture_output=True, 
                                   creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if os.path.exists(temp_video_path):
                os.remove(temp_video_path)
            
            if process.returncode != 0:
                error_msg = process.stderr.decode('utf-8', errors='ignore')
                print(f"éŸ³å£°çµåˆã‚¨ãƒ©ãƒ¼: {error_msg}")
                return False
            
            print("éŸ³å£°çµåˆ+ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except Exception as e:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if 'temp_video_path' in locals() and os.path.exists(temp_video_path):
                try:
                    os.remove(temp_video_path)
                except:
                    pass
            print(f"OpenCVãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_temp_video_with_perspective(self, temp_path):
        """å°å½¢è£œæ­£ã‚’é©ç”¨ã—ãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆéŸ³å£°ä»˜ãï¼‰"""
        try:
            src_points = np.float32([
                [float(self.point_entries[0][0].get()), float(self.point_entries[0][1].get())],  # å·¦ä¸Š
                [float(self.point_entries[1][0].get()), float(self.point_entries[1][1].get())],  # å³ä¸Š
                [float(self.point_entries[2][0].get()), float(self.point_entries[2][1].get())],  # å·¦ä¸‹
                [float(self.point_entries[3][0].get()), float(self.point_entries[3][1].get())]   # å³ä¸‹
            ])
            dst_points = np.float32([
                [0, 0],
                [self.video_info['width'], 0],
                [0, self.video_info['height']],
                [self.video_info['width'], self.video_info['height']]
            ])
            
            cap = cv2.VideoCapture(self.video_path)
            
            # éŸ³å£°ä»˜ãã§å‡ºåŠ›ã™ã‚‹ãŸã‚ã€ffmpegã‚’ä½¿ç”¨
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_temp = temp_path.replace('.mp4', '_video_only.mp4')
            out = cv2.VideoWriter(video_temp, fourcc, self.video_info['fps'],
                                (self.video_info['width'], self.video_info['height']))
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            processed_frames = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # é€è¦–å¤‰æ›ã‚’é©ç”¨
                matrix = cv2.getPerspectiveTransform(src_points, dst_points)
                corrected = cv2.warpPerspective(frame, matrix, 
                                              (self.video_info['width'], self.video_info['height']))
                out.write(corrected)
                
                processed_frames += 1
                if processed_frames % 30 == 0:  # 30ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«æ›´æ–°
                    progress = (processed_frames / total_frames) * 50  # å°å½¢è£œæ­£ã¯å…¨ä½“ã®50%
                    self.root.after(0, lambda p=progress: self.update_progress(p, "å°å½¢è£œæ­£ä¸­..."))
            
            cap.release()
            out.release()
            
            # éŸ³å£°ã‚’å…ƒã®å‹•ç”»ã‹ã‚‰æŠ½å‡ºã—ã¦çµåˆ
            audio_cmd = ['ffmpeg', '-y', 
                        '-i', video_temp,  # æ˜ åƒã®ã¿
                        '-i', self.video_path,  # å…ƒå‹•ç”»ï¼ˆéŸ³å£°ç”¨ï¼‰
                        '-c:v', 'copy',  # æ˜ åƒã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼
                        '-c:a', 'copy',  # éŸ³å£°ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼
                        '-map', '0:v:0',  # 1ç•ªç›®ã®å…¥åŠ›ã®æ˜ åƒ
                        '-map', '1:a:0',  # 2ç•ªç›®ã®å…¥åŠ›ã®éŸ³å£°
                        temp_path]
            
            process = subprocess.run(audio_cmd, capture_output=True, 
                                   creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if os.path.exists(video_temp):
                os.remove(video_temp)
            
            return process.returncode == 0
            
        except Exception as e:
            print(f"å°å½¢è£œæ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def process_video(self):
        if not self.video_path:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            return
        
        # ä¿å­˜å…ˆã‚’é¸æŠ
        output_path = filedialog.asksaveasfilename(
            title="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®š",
            defaultextension=".mp4",
            filetypes=[("MP4ãƒ•ã‚¡ã‚¤ãƒ«", "*.mp4")]
        )
        if not output_path:
            return
        
        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ã‚’é–‹å§‹
        thread = threading.Thread(target=self._process_video_thread, args=(output_path,))
        thread.daemon = True
        thread.start()
    
    def _process_video_thread(self, output_path):
        try:
            start_time = self.get_time_in_seconds(self.start_h, self.start_m, self.start_s, self.start_ms)
            end_time = self.get_time_in_seconds(self.end_h, self.end_m, self.end_s, self.end_ms)
            
            if start_time >= end_time:
                self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "é–‹å§‹æ™‚é–“ã¯çµ‚äº†æ™‚é–“ã‚ˆã‚Šå‰ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"))
                return
            
            if end_time > self.video_info['duration']:
                self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "çµ‚äº†æ™‚é–“ãŒå‹•ç”»ã®é•·ã•ã‚’è¶…ãˆã¦ã„ã¾ã™"))
                return
            
            # é€²æ—è¡¨ç¤ºã‚’æ›´æ–°
            self.root.after(0, lambda: self.progress_label.config(text="å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™..."))
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼è¨­å®šã‚’å–å¾—
            encoder, quality_settings = self.get_encoder_settings()
            
            success = False
            
            if encoder == 'opencv':
                # OpenCVã§å‡¦ç†
                success = self.process_video_opencv(output_path, start_time, end_time, quality_settings)
            else:
                # ffmpegã§å‡¦ç†
                success = self.process_video_ffmpeg(output_path, start_time, end_time, encoder, quality_settings)
            
            if success:
                # å®Œäº†é€šçŸ¥
                self.root.after(0, lambda: self.progress_label.config(text="å®Œäº†ï¼"))
                self.root.after(0, lambda: self.progress_bar.config(value=100))
                self.root.after(0, lambda: messagebox.showinfo("å®Œäº†", f"å‹•ç”»ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\nã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: {self.encoder_var.get()}"))
            else:
                raise Exception("å‹•ç”»å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
        except Exception as e:
            self.root.after(0, lambda: messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"))
            self.root.after(0, lambda: self.progress_label.config(text="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"))
    
    def parse_time_to_seconds(self, time_str):
        """HH:MM:SS.sså½¢å¼ã®æ™‚é–“ã‚’ç§’ã«å¤‰æ›"""
        try:
            parts = time_str.split(':')
            hours = int(parts[0])
            minutes = int(parts[1])
            seconds = float(parts[2])
            return hours * 3600 + minutes * 60 + seconds
        except:
            return 0
    
    def update_progress(self, progress, message):
        self.progress_bar.config(value=min(progress, 100))
        self.progress_label.config(text=message)

if __name__ == "__main__":
    root = tk.Tk()
    app = VideoEditor(root)
    root.mainloop()